<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=pYEwJuzr3ZMDX2Y1syVbvx06if6osnyAslCuLPPf50A');ol.lst-kix_sxslfcnktzg1-1{list-style-type:none}ol.lst-kix_sxslfcnktzg1-2{list-style-type:none}ol.lst-kix_sxslfcnktzg1-0{list-style-type:none}ol.lst-kix_sxslfcnktzg1-5{list-style-type:none}ol.lst-kix_sxslfcnktzg1-6{list-style-type:none}ol.lst-kix_sxslfcnktzg1-3{list-style-type:none}ol.lst-kix_sxslfcnktzg1-4{list-style-type:none}ol.lst-kix_sxslfcnktzg1-7{list-style-type:none}ol.lst-kix_sxslfcnktzg1-8{list-style-type:none}.lst-kix_pmz5s3rqvha4-7>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-7}ol.lst-kix_sxslfcnktzg1-7.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-7 0}ol.lst-kix_pmz5s3rqvha4-4.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-4 0}.lst-kix_sxslfcnktzg1-0>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-0}ol.lst-kix_b7drba6x2oxi-6.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-6 0}.lst-kix_b7drba6x2oxi-6>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-6}ol.lst-kix_b7drba6x2oxi-0.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-0 0}.lst-kix_vocg9dsh5o45-2>li:before{content:"\0025a0  "}.lst-kix_u24g1rtzxwjj-5>li:before{content:"\0025a0  "}.lst-kix_u24g1rtzxwjj-6>li:before{content:"\0025cf  "}.lst-kix_vocg9dsh5o45-3>li:before{content:"\0025cf  "}.lst-kix_u24g1rtzxwjj-2>li:before{content:"\0025a0  "}.lst-kix_vocg9dsh5o45-0>li:before{content:"\0025cf  "}.lst-kix_u24g1rtzxwjj-3>li:before{content:"\0025cf  "}.lst-kix_u24g1rtzxwjj-4>li:before{content:"\0025cb  "}.lst-kix_vocg9dsh5o45-8>li:before{content:"\0025a0  "}.lst-kix_vocg9dsh5o45-1>li:before{content:"\0025cb  "}.lst-kix_vocg9dsh5o45-7>li:before{content:"\0025cb  "}.lst-kix_vocg9dsh5o45-6>li:before{content:"\0025cf  "}.lst-kix_vocg9dsh5o45-4>li:before{content:"\0025cb  "}.lst-kix_u24g1rtzxwjj-7>li:before{content:"\0025cb  "}.lst-kix_u24g1rtzxwjj-8>li:before{content:"\0025a0  "}.lst-kix_vocg9dsh5o45-5>li:before{content:"\0025a0  "}ol.lst-kix_sxslfcnktzg1-2.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-2 0}ul.lst-kix_a102oay53e4d-5{list-style-type:none}ul.lst-kix_a102oay53e4d-6{list-style-type:none}ul.lst-kix_a102oay53e4d-3{list-style-type:none}ul.lst-kix_a102oay53e4d-4{list-style-type:none}ul.lst-kix_a102oay53e4d-1{list-style-type:none}ul.lst-kix_a102oay53e4d-2{list-style-type:none}ul.lst-kix_a102oay53e4d-0{list-style-type:none}.lst-kix_b7drba6x2oxi-4>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-4}ol.lst-kix_b7drba6x2oxi-1.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-1 0}ul.lst-kix_a102oay53e4d-7{list-style-type:none}ul.lst-kix_a102oay53e4d-8{list-style-type:none}ol.lst-kix_sxslfcnktzg1-8.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-8 0}ol.lst-kix_sxslfcnktzg1-1.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-1 0}.lst-kix_u24g1rtzxwjj-1>li:before{content:"\0025cb  "}.lst-kix_u24g1rtzxwjj-0>li:before{content:"\0025cf  "}ul.lst-kix_u24g1rtzxwjj-6{list-style-type:none}ul.lst-kix_u24g1rtzxwjj-5{list-style-type:none}ul.lst-kix_u24g1rtzxwjj-4{list-style-type:none}ul.lst-kix_u24g1rtzxwjj-3{list-style-type:none}.lst-kix_8zk7keh8qa6c-2>li:before{content:"\0025a0  "}.lst-kix_8zk7keh8qa6c-4>li:before{content:"\0025cb  "}ul.lst-kix_u24g1rtzxwjj-8{list-style-type:none}.lst-kix_sxslfcnktzg1-4>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-4}ul.lst-kix_u24g1rtzxwjj-7{list-style-type:none}ul.lst-kix_1rj01lbgs74w-8{list-style-type:none}ul.lst-kix_1rj01lbgs74w-7{list-style-type:none}ul.lst-kix_1rj01lbgs74w-4{list-style-type:none}ul.lst-kix_u24g1rtzxwjj-2{list-style-type:none}ul.lst-kix_vx89dbfa2t45-0{list-style-type:none}ul.lst-kix_1rj01lbgs74w-3{list-style-type:none}ul.lst-kix_u24g1rtzxwjj-1{list-style-type:none}ul.lst-kix_vx89dbfa2t45-1{list-style-type:none}ul.lst-kix_1rj01lbgs74w-6{list-style-type:none}ul.lst-kix_u24g1rtzxwjj-0{list-style-type:none}ul.lst-kix_1rj01lbgs74w-5{list-style-type:none}ul.lst-kix_1rj01lbgs74w-0{list-style-type:none}ul.lst-kix_8zk7keh8qa6c-1{list-style-type:none}ul.lst-kix_8zk7keh8qa6c-0{list-style-type:none}ul.lst-kix_1rj01lbgs74w-2{list-style-type:none}ul.lst-kix_8zk7keh8qa6c-3{list-style-type:none}.lst-kix_y58mt09ypq8c-2>li:before{content:"\0025a0  "}.lst-kix_y58mt09ypq8c-4>li:before{content:"\0025cb  "}ul.lst-kix_1rj01lbgs74w-1{list-style-type:none}ul.lst-kix_8zk7keh8qa6c-2{list-style-type:none}.lst-kix_pmz5s3rqvha4-1>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-1}.lst-kix_sxslfcnktzg1-3>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-3}ul.lst-kix_8zk7keh8qa6c-8{list-style-type:none}ol.lst-kix_pmz5s3rqvha4-0.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-0 0}.lst-kix_y58mt09ypq8c-0>li:before{content:"\0025cf  "}.lst-kix_8zk7keh8qa6c-6>li:before{content:"\0025cf  "}.lst-kix_8zk7keh8qa6c-8>li:before{content:"\0025a0  "}ul.lst-kix_8zk7keh8qa6c-5{list-style-type:none}ul.lst-kix_8zk7keh8qa6c-4{list-style-type:none}ul.lst-kix_8zk7keh8qa6c-7{list-style-type:none}ul.lst-kix_8zk7keh8qa6c-6{list-style-type:none}ul.lst-kix_byq40nhaw2di-7{list-style-type:none}ul.lst-kix_byq40nhaw2di-6{list-style-type:none}ul.lst-kix_byq40nhaw2di-5{list-style-type:none}ul.lst-kix_byq40nhaw2di-4{list-style-type:none}ul.lst-kix_byq40nhaw2di-3{list-style-type:none}ul.lst-kix_byq40nhaw2di-2{list-style-type:none}ul.lst-kix_byq40nhaw2di-1{list-style-type:none}ul.lst-kix_byq40nhaw2di-0{list-style-type:none}ol.lst-kix_sxslfcnktzg1-0.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-0 0}.lst-kix_y58mt09ypq8c-6>li:before{content:"\0025cf  "}.lst-kix_sxslfcnktzg1-5>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-5}.lst-kix_b7drba6x2oxi-0>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-0,decimal) ". "}ul.lst-kix_byq40nhaw2di-8{list-style-type:none}ul.lst-kix_vx89dbfa2t45-4{list-style-type:none}ul.lst-kix_vx89dbfa2t45-5{list-style-type:none}ul.lst-kix_vx89dbfa2t45-2{list-style-type:none}ul.lst-kix_vx89dbfa2t45-3{list-style-type:none}ul.lst-kix_vx89dbfa2t45-8{list-style-type:none}.lst-kix_pmz5s3rqvha4-3>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-3}ul.lst-kix_vx89dbfa2t45-6{list-style-type:none}ul.lst-kix_vx89dbfa2t45-7{list-style-type:none}.lst-kix_b7drba6x2oxi-2>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-2,lower-roman) ". "}.lst-kix_b7drba6x2oxi-6>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-6,decimal) ". "}.lst-kix_y58mt09ypq8c-8>li:before{content:"\0025a0  "}.lst-kix_b7drba6x2oxi-4>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-4,lower-latin) ". "}.lst-kix_b7drba6x2oxi-8>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-8}.lst-kix_sxslfcnktzg1-3>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-3,decimal) ". "}.lst-kix_sxslfcnktzg1-5>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-5,lower-roman) ". "}.lst-kix_b7drba6x2oxi-2>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-2}.lst-kix_sxslfcnktzg1-1>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-1,lower-latin) ". "}.lst-kix_b7drba6x2oxi-8>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-8,lower-roman) ". "}ul.lst-kix_vocg9dsh5o45-5{list-style-type:none}ul.lst-kix_vocg9dsh5o45-6{list-style-type:none}ul.lst-kix_vocg9dsh5o45-7{list-style-type:none}ul.lst-kix_vocg9dsh5o45-8{list-style-type:none}ul.lst-kix_vocg9dsh5o45-1{list-style-type:none}ul.lst-kix_vocg9dsh5o45-2{list-style-type:none}ul.lst-kix_vocg9dsh5o45-3{list-style-type:none}.lst-kix_pmz5s3rqvha4-7>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-7,lower-latin) ". "}ul.lst-kix_vocg9dsh5o45-4{list-style-type:none}.lst-kix_byq40nhaw2di-0>li:before{content:"\0025cf  "}ul.lst-kix_vocg9dsh5o45-0{list-style-type:none}.lst-kix_a102oay53e4d-7>li:before{content:"\0025cb  "}.lst-kix_byq40nhaw2di-2>li:before{content:"\0025a0  "}.lst-kix_vx89dbfa2t45-3>li:before{content:"\0025cf  "}.lst-kix_byq40nhaw2di-4>li:before{content:"\0025cb  "}.lst-kix_pmz5s3rqvha4-2>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-2}.lst-kix_pmz5s3rqvha4-8>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-8}.lst-kix_vx89dbfa2t45-7>li:before{content:"\0025cb  "}.lst-kix_vx89dbfa2t45-5>li:before{content:"\0025a0  "}.lst-kix_byq40nhaw2di-6>li:before{content:"\0025cf  "}.lst-kix_pmz5s3rqvha4-5>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-5,lower-roman) ". "}.lst-kix_pmz5s3rqvha4-3>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-3,decimal) ". "}.lst-kix_byq40nhaw2di-8>li:before{content:"\0025a0  "}.lst-kix_sxslfcnktzg1-7>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-7,lower-latin) ". "}.lst-kix_8zk7keh8qa6c-0>li:before{content:"\0025cf  "}.lst-kix_vx89dbfa2t45-1>li:before{content:"\0025cb  "}.lst-kix_pmz5s3rqvha4-1>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-1,lower-latin) ". "}.lst-kix_qibn6ao0a1s7-8>li:before{content:"\0025a0  "}.lst-kix_qibn6ao0a1s7-6>li:before{content:"\0025cf  "}ul.lst-kix_y58mt09ypq8c-4{list-style-type:none}ul.lst-kix_y58mt09ypq8c-5{list-style-type:none}ul.lst-kix_y58mt09ypq8c-6{list-style-type:none}ul.lst-kix_y58mt09ypq8c-7{list-style-type:none}ul.lst-kix_y58mt09ypq8c-0{list-style-type:none}ol.lst-kix_sxslfcnktzg1-4.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-4 0}ul.lst-kix_y58mt09ypq8c-1{list-style-type:none}.lst-kix_qibn6ao0a1s7-7>li:before{content:"\0025cb  "}ul.lst-kix_y58mt09ypq8c-2{list-style-type:none}ol.lst-kix_pmz5s3rqvha4-7.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-7 0}ul.lst-kix_y58mt09ypq8c-3{list-style-type:none}.lst-kix_qibn6ao0a1s7-2>li:before{content:"\0025a0  "}.lst-kix_qibn6ao0a1s7-0>li:before{content:"\0025cf  "}.lst-kix_qibn6ao0a1s7-4>li:before{content:"\0025cb  "}.lst-kix_qibn6ao0a1s7-1>li:before{content:"\0025cb  "}.lst-kix_qibn6ao0a1s7-5>li:before{content:"\0025a0  "}.lst-kix_b7drba6x2oxi-5>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-5}.lst-kix_qibn6ao0a1s7-3>li:before{content:"\0025cf  "}.lst-kix_a102oay53e4d-1>li:before{content:"\0025cb  "}.lst-kix_a102oay53e4d-0>li:before{content:"\0025cf  "}.lst-kix_a102oay53e4d-2>li:before{content:"\0025a0  "}ol.lst-kix_b7drba6x2oxi-2{list-style-type:none}.lst-kix_a102oay53e4d-5>li:before{content:"\0025a0  "}ol.lst-kix_b7drba6x2oxi-1{list-style-type:none}ol.lst-kix_b7drba6x2oxi-4{list-style-type:none}.lst-kix_a102oay53e4d-4>li:before{content:"\0025cb  "}ol.lst-kix_b7drba6x2oxi-3{list-style-type:none}.lst-kix_a102oay53e4d-3>li:before{content:"\0025cf  "}ol.lst-kix_b7drba6x2oxi-0{list-style-type:none}ol.lst-kix_pmz5s3rqvha4-1.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-1 0}.lst-kix_b7drba6x2oxi-3>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-3}ul.lst-kix_qibn6ao0a1s7-0{list-style-type:none}ol.lst-kix_b7drba6x2oxi-6{list-style-type:none}ul.lst-kix_qibn6ao0a1s7-2{list-style-type:none}ol.lst-kix_b7drba6x2oxi-5{list-style-type:none}ul.lst-kix_qibn6ao0a1s7-1{list-style-type:none}ol.lst-kix_b7drba6x2oxi-8{list-style-type:none}ul.lst-kix_qibn6ao0a1s7-4{list-style-type:none}.lst-kix_sxslfcnktzg1-8>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-8}ol.lst-kix_b7drba6x2oxi-7{list-style-type:none}ul.lst-kix_qibn6ao0a1s7-3{list-style-type:none}ul.lst-kix_qibn6ao0a1s7-6{list-style-type:none}ul.lst-kix_qibn6ao0a1s7-5{list-style-type:none}ul.lst-kix_qibn6ao0a1s7-8{list-style-type:none}ol.lst-kix_b7drba6x2oxi-3.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-3 0}.lst-kix_pmz5s3rqvha4-6>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-6}ul.lst-kix_qibn6ao0a1s7-7{list-style-type:none}ul.lst-kix_xdasi2krd8wx-2{list-style-type:none}.lst-kix_1rj01lbgs74w-5>li:before{content:"\0025a0  "}.lst-kix_1rj01lbgs74w-6>li:before{content:"\0025cf  "}ul.lst-kix_xdasi2krd8wx-3{list-style-type:none}ul.lst-kix_xdasi2krd8wx-4{list-style-type:none}ul.lst-kix_xdasi2krd8wx-5{list-style-type:none}ul.lst-kix_xdasi2krd8wx-0{list-style-type:none}ul.lst-kix_xdasi2krd8wx-1{list-style-type:none}.lst-kix_1rj01lbgs74w-1>li:before{content:"\0025cb  "}ul.lst-kix_xdasi2krd8wx-6{list-style-type:none}.lst-kix_1rj01lbgs74w-0>li:before{content:"\0025cf  "}.lst-kix_1rj01lbgs74w-7>li:before{content:"\0025cb  "}.lst-kix_1rj01lbgs74w-8>li:before{content:"\0025a0  "}ul.lst-kix_xdasi2krd8wx-7{list-style-type:none}ul.lst-kix_xdasi2krd8wx-8{list-style-type:none}ol.lst-kix_b7drba6x2oxi-2.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-2 0}ol.lst-kix_pmz5s3rqvha4-8.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-8 0}.lst-kix_pmz5s3rqvha4-4>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-4}.lst-kix_sxslfcnktzg1-6>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-6}ol.lst-kix_pmz5s3rqvha4-5{list-style-type:none}ol.lst-kix_pmz5s3rqvha4-6{list-style-type:none}ol.lst-kix_pmz5s3rqvha4-3{list-style-type:none}ol.lst-kix_pmz5s3rqvha4-4{list-style-type:none}.lst-kix_xdasi2krd8wx-7>li:before{content:"\0025cb  "}ol.lst-kix_pmz5s3rqvha4-7{list-style-type:none}ol.lst-kix_pmz5s3rqvha4-8{list-style-type:none}.lst-kix_xdasi2krd8wx-4>li:before{content:"\0025cb  "}.lst-kix_xdasi2krd8wx-8>li:before{content:"\0025a0  "}.lst-kix_b7drba6x2oxi-7>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-7}ol.lst-kix_pmz5s3rqvha4-2.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-2 0}.lst-kix_xdasi2krd8wx-5>li:before{content:"\0025a0  "}ol.lst-kix_pmz5s3rqvha4-1{list-style-type:none}.lst-kix_b7drba6x2oxi-1>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-1}ol.lst-kix_pmz5s3rqvha4-2{list-style-type:none}ol.lst-kix_pmz5s3rqvha4-0{list-style-type:none}.lst-kix_xdasi2krd8wx-6>li:before{content:"\0025cf  "}.lst-kix_xdasi2krd8wx-1>li:before{content:"\0025cb  "}ul.lst-kix_y58mt09ypq8c-8{list-style-type:none}.lst-kix_xdasi2krd8wx-0>li:before{content:"\0025cf  "}.lst-kix_1rj01lbgs74w-2>li:before{content:"\0025a0  "}.lst-kix_xdasi2krd8wx-3>li:before{content:"\0025cf  "}.lst-kix_1rj01lbgs74w-3>li:before{content:"\0025cf  "}.lst-kix_1rj01lbgs74w-4>li:before{content:"\0025cb  "}ol.lst-kix_b7drba6x2oxi-8.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-8 0}.lst-kix_xdasi2krd8wx-2>li:before{content:"\0025a0  "}.lst-kix_8zk7keh8qa6c-1>li:before{content:"\0025cb  "}.lst-kix_8zk7keh8qa6c-5>li:before{content:"\0025a0  "}ol.lst-kix_pmz5s3rqvha4-3.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-3 0}.lst-kix_8zk7keh8qa6c-3>li:before{content:"\0025cf  "}.lst-kix_y58mt09ypq8c-3>li:before{content:"\0025cf  "}ol.lst-kix_b7drba6x2oxi-7.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-7 0}ol.lst-kix_b7drba6x2oxi-4.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-4 0}.lst-kix_y58mt09ypq8c-1>li:before{content:"\0025cb  "}.lst-kix_8zk7keh8qa6c-7>li:before{content:"\0025cb  "}.lst-kix_y58mt09ypq8c-5>li:before{content:"\0025a0  "}.lst-kix_sxslfcnktzg1-2>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-2}.lst-kix_b7drba6x2oxi-0>li{counter-increment:lst-ctn-kix_b7drba6x2oxi-0}.lst-kix_pmz5s3rqvha4-0>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-0}.lst-kix_b7drba6x2oxi-1>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-1,lower-latin) ". "}.lst-kix_b7drba6x2oxi-3>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-3,decimal) ". "}ol.lst-kix_sxslfcnktzg1-3.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-3 0}.lst-kix_y58mt09ypq8c-7>li:before{content:"\0025cb  "}ol.lst-kix_sxslfcnktzg1-6.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-6 0}.lst-kix_b7drba6x2oxi-5>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-5,lower-roman) ". "}.lst-kix_sxslfcnktzg1-4>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-4,lower-latin) ". "}.lst-kix_sxslfcnktzg1-7>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-7}ol.lst-kix_b7drba6x2oxi-5.start{counter-reset:lst-ctn-kix_b7drba6x2oxi-5 0}.lst-kix_sxslfcnktzg1-8>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-8,lower-roman) ". "}.lst-kix_b7drba6x2oxi-7>li:before{content:"" counter(lst-ctn-kix_b7drba6x2oxi-7,lower-latin) ". "}.lst-kix_sxslfcnktzg1-2>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-2,lower-roman) ". "}.lst-kix_a102oay53e4d-8>li:before{content:"\0025a0  "}.lst-kix_pmz5s3rqvha4-6>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-6,decimal) ". "}.lst-kix_pmz5s3rqvha4-8>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-8,lower-roman) ". "}.lst-kix_vx89dbfa2t45-8>li:before{content:"\0025a0  "}.lst-kix_sxslfcnktzg1-0>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-0,decimal) ". "}.lst-kix_byq40nhaw2di-1>li:before{content:"\0025cb  "}.lst-kix_a102oay53e4d-6>li:before{content:"\0025cf  "}ol.lst-kix_pmz5s3rqvha4-5.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-5 0}.lst-kix_vx89dbfa2t45-2>li:before{content:"\0025a0  "}.lst-kix_byq40nhaw2di-5>li:before{content:"\0025a0  "}ol.lst-kix_sxslfcnktzg1-5.start{counter-reset:lst-ctn-kix_sxslfcnktzg1-5 0}.lst-kix_pmz5s3rqvha4-0>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-0,decimal) ". "}.lst-kix_vx89dbfa2t45-0>li:before{content:"\0025cf  "}.lst-kix_vx89dbfa2t45-4>li:before{content:"\0025cb  "}.lst-kix_byq40nhaw2di-3>li:before{content:"\0025cf  "}.lst-kix_byq40nhaw2di-7>li:before{content:"\0025cb  "}.lst-kix_vx89dbfa2t45-6>li:before{content:"\0025cf  "}.lst-kix_pmz5s3rqvha4-4>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-4,lower-latin) ". "}.lst-kix_pmz5s3rqvha4-5>li{counter-increment:lst-ctn-kix_pmz5s3rqvha4-5}ol.lst-kix_pmz5s3rqvha4-6.start{counter-reset:lst-ctn-kix_pmz5s3rqvha4-6 0}.lst-kix_pmz5s3rqvha4-2>li:before{content:"" counter(lst-ctn-kix_pmz5s3rqvha4-2,lower-roman) ". "}.lst-kix_sxslfcnktzg1-6>li:before{content:"" counter(lst-ctn-kix_sxslfcnktzg1-6,decimal) ". "}.lst-kix_sxslfcnktzg1-1>li{counter-increment:lst-ctn-kix_sxslfcnktzg1-1}ol{margin:0;padding:0}table td,table th{padding:0}.c67{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#ffffff;border-top-width:1pt;border-right-width:1pt;border-left-color:#ffffff;vertical-align:top;border-right-color:#ffffff;border-left-width:1pt;border-top-style:solid;background-color:#2b2b2b;border-left-style:solid;border-bottom-width:1pt;width:229.5pt;border-top-color:#ffffff;border-bottom-style:solid}.c68{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#2b2b2b;border-left-style:solid;border-bottom-width:1pt;width:585pt;border-top-color:#000000;border-bottom-style:solid}.c65{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;background-color:#2b2b2b;border-left-style:solid;border-bottom-width:0pt;width:585pt;border-top-color:#000000;border-bottom-style:solid}.c57{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:237pt;border-top-color:#000000;border-bottom-style:solid}.c59{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:585.8pt;border-top-color:#000000;border-bottom-style:solid}.c54{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:190.5pt;border-top-color:#000000;border-bottom-style:solid}.c48{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:395.2pt;border-top-color:#000000;border-bottom-style:solid}.c32{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:292.5pt;border-top-color:#000000;border-bottom-style:solid}.c29{-webkit-text-decoration-skip:none;color:#0366d6;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Arial";font-style:normal}.c4{margin-left:4.5pt;padding-top:3pt;list-style-position:inside;text-indent:45pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c2{background-color:#2b2b2b;color:#a9b7c6;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c34{background-color:#f6f8fa;color:#24292e;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Consolas";font-style:normal}.c0{margin-left:4.5pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c20{background-color:#2b2b2b;color:#808080;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c21{background-color:#ffffff;-webkit-text-decoration-skip:none;color:#02b3e4;text-decoration:underline;text-decoration-skip-ink:none;font-size:10.5pt;font-style:italic}.c53{color:#24292e;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16.5pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c31{padding-top:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c37{padding-top:0pt;padding-bottom:9pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c28{color:#58646d;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c14{color:#24292e;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:23pt;font-family:"Arial";font-style:normal}.c10{margin-left:4.5pt;padding-top:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c3{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c58{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c39{background-color:#ffffff;-webkit-text-decoration-skip:none;color:#02b3e4;text-decoration:underline;text-decoration-skip-ink:none;font-size:10.5pt}.c1{padding-top:0pt;padding-bottom:11pt;line-height:1.3636363636363635;orphans:2;widows:2;text-align:left}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c46{padding-top:0pt;padding-bottom:12pt;line-height:1.45;orphans:2;widows:2;text-align:left}.c61{padding-top:0pt;padding-bottom:9pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c38{padding-top:18pt;padding-bottom:12pt;line-height:1.15;page-break-after:avoid;text-align:left}.c25{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c30{font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c7{margin-left:4.5pt;padding-top:18pt;padding-bottom:12pt;line-height:1.0;text-align:left}.c49{text-decoration-skip-ink:none;font-size:12pt;-webkit-text-decoration-skip:none;color:#0366d6;text-decoration:underline}.c26{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c66{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c27{padding-top:16pt;padding-bottom:12pt;line-height:1.45;page-break-after:avoid;text-align:left}.c24{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c23{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c70{font-size:12pt;font-family:"Roboto";color:#212121;font-weight:400}.c41{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c63{font-size:10pt;font-family:"Consolas";color:#24292e;font-weight:400}.c22{padding-top:24pt;padding-bottom:12pt;line-height:1.25;text-align:left}.c11{background-color:#2b2b2b;font-size:8pt;color:#cc7832}.c13{background-color:#2b2b2b;font-size:8pt;color:#aa4926}.c35{background-color:#2b2b2b;font-size:8pt;color:#94558d}.c60{background-color:#f9f9f9;font-size:10.5pt;color:#303030}.c33{background-color:#2b2b2b;font-size:8pt;color:#6897bb}.c16{background-color:#2b2b2b;font-size:8pt;color:#a9b7c6}.c43{background-color:#ffffff;color:#58646d;font-size:10.5pt}.c17{background-color:#2b2b2b;font-size:8pt;color:#6a8759}.c52{border-spacing:0;border-collapse:collapse;margin-right:auto}.c45{background-color:#2b2b2b;font-size:8pt;color:#8888c6}.c64{color:#58646d;font-size:10.5pt}.c9{color:#24292e;font-size:12pt}.c50{padding:0;margin:0}.c42{color:#808080;font-size:8pt}.c8{color:inherit;text-decoration:inherit}.c56{list-style-position:inside;text-indent:45pt}.c47{max-width:585pt;padding:72pt 18pt 72pt 9pt}.c55{margin-left:36pt;padding-left:0pt}.c62{font-weight:700}.c6{height:108pt}.c36{height:0pt}.c51{height:21pt}.c18{margin-left:4.5pt}.c44{background-color:#ffffff}.c71{margin-left:1.5pt}.c40{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c44 c47"><h1 class="c22 c18" id="h.sl8q8y4gq0tb"><span class="c14">CarND-Path-Planning-Project</span></h1><h1 class="c18 c22" id="h.3gc3ewas5i1j"><span class="c14">Semantic Segmentation</span></h1><h3 class="c7" id="h.ihhrvp3uctp9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 576.00px; height: 160.00px;"><img alt="" src="images/image9.png" style="width: 576.00px; height: 160.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h3><h2 class="c7" id="h.8k1oepb1840z"><span class="c19">Introduction</span></h2><p class="c10"><span>The purpose of this project to segment road lanes by implementing correctly image segmentation </span><span class="c9">Fully Convolutional Network (FCN) </span><span>as specified in the </span><span class="c23"><a class="c8" href="https://www.google.com/url?q=https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&amp;sa=D&amp;ust=1522472818100000">Fully Convolutional Networks for Semantic Segmentation</a></span><span class="c12">&nbsp;paper. </span></p><h2 class="c18 c38" id="h.geqv5l3926o4"><span class="c19">Summary </span></h2><ul class="c50 lst-kix_vx89dbfa2t45-0 start"><li class="c5 c55"><span class="c12">In this projects I I loaded VGG16 pretrained model and finetune VGG model to do lane segmentation to human-level accuracy. </span></li><li class="c5 c55"><span class="c12">Correctly combined new layers with VGG16 to construct your model.</span></li><li class="c5 c55"><span>Skip connections are used from previous vgg_layer3_out and vgg_layer4_out layers. The authors of </span><span class="c23"><a class="c8" href="https://www.google.com/url?q=https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&amp;sa=D&amp;ust=1522472818101000">original paper</a></span><span class="c12">&nbsp;highly suggest to use these skip connections to improve segmentation accuracy.</span></li><li class="c5 c55"><span class="c12">Correctly added Deconvolution or Transpose Strided layers on top of provided VGG model.</span></li><li class="c5 c55"><span class="c12">Optimized graph parameters and train </span></li></ul><p class="c37"><span class="c30 c64"></span></p><h2 class="c24" id="h.cdrdlyjkurgu"><span class="c19">Below is a visualization of the projects architecture: </span></h2><p class="c61"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 780.00px; height: 464.00px;"><img alt="" src="images/image11.png" style="width: 780.00px; height: 464.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c7" id="h.ch0qbzojsdsi"><span class="c53">Setup</span></h3><h3 class="c7" id="h.kmmkbog2bz1y"><span class="c3">Frameworks and Packages</span></h3><p class="c10"><span class="c30 c9">Make sure you have the following is installed:</span></p><ul class="c50 lst-kix_1rj01lbgs74w-0 start"><li class="c10 c56"><span class="c29"><a class="c8" href="https://www.google.com/url?q=https://www.python.org/&amp;sa=D&amp;ust=1522472818103000">Python 3</a></span></li><li class="c4"><span class="c29"><a class="c8" href="https://www.google.com/url?q=https://www.tensorflow.org/&amp;sa=D&amp;ust=1522472818103000">TensorFlow</a></span></li><li class="c4"><span class="c29"><a class="c8" href="https://www.google.com/url?q=http://www.numpy.org/&amp;sa=D&amp;ust=1522472818103000">NumPy</a></span></li><li class="c4"><span class="c29"><a class="c8" href="https://www.google.com/url?q=https://www.scipy.org/&amp;sa=D&amp;ust=1522472818104000">SciPy</a></span></li></ul><h3 class="c7" id="h.h67a7u9i51oy"><span class="c3">Dataset</span></h3><p class="c10"><span class="c9">Download the </span><span class="c49"><a class="c8" href="https://www.google.com/url?q=http://www.cvlibs.net/datasets/kitti/eval_road.php&amp;sa=D&amp;ust=1522472818104000">Kitti Road dataset</a></span><span class="c9">&nbsp;from </span><span class="c49"><a class="c8" href="https://www.google.com/url?q=http://www.cvlibs.net/download.php?file%3Ddata_road.zip&amp;sa=D&amp;ust=1522472818104000">here</a></span><span class="c9">. Extract the dataset in the </span><span class="c63">data</span><span class="c9">&nbsp;folder. This will create the folder </span><span class="c63">data_road</span><span class="c9 c30">&nbsp;with all the training a test images.</span></p><h2 class="c7" id="h.hu03jrh0lj2d"><span class="c58">Run</span></h2><p class="c10"><span class="c30 c9">Run the following command to run the project:</span></p><p class="c18 c46"><span class="c34">python main.py</span></p><p class="c5"><span>To run this code you would need a GPU with at least 6GB of memory, because I did not have one with me I used </span><span class="c23"><a class="c8" href="https://www.google.com/url?q=http://www.floydhub.com&amp;sa=D&amp;ust=1522472818106000">floyhub</a></span><span class="c12">.</span></p><p class="c5 c40"><span class="c12"></span></p><p class="c5"><span class="c12">To run the project under floydhub type this line in your command line after you upload your data road dataset and vgg model:</span></p><p class="c5 c40"><span class="c12"></span></p><p class="c5"><span class="c12">floyd run --gpu --env tensorflow-1.3 --data USERNAME/datasets/data_road/1:/data_road</span></p><p class="c5"><span class="c12">--data USERNAME/datasets/pretrained_vgg/1:/pretrained_vgg &quot;python main.py&quot;</span></p><p class="c46 c18 c40"><span class="c34"></span></p><h3 class="c18 c27" id="h.cmmowlzhwcy8"><span class="c62">Project walk through:</span></h3><ol class="c50 lst-kix_b7drba6x2oxi-0 start" start="1"><li class="c25 c55"><h3 id="h.8nd9kbk7z486" style="display:inline"><span class="c3">Load the pretrained vgg model</span></h3></li></ol><p class="c5 c18"><span>F</span><span class="c12">unction load_vgg is implemented correctly to load (see main.py ln54). It loads the model VGG from a SavedModel as specified by &nbsp;tags &lsquo;vgg16&rsquo; and it saves it in the specified path vgg_path and with tf.get_default_graph() we get the graph with the loaded context.</span></p><p class="c0"><span class="c12"></span></p><a id="t.07784d1a38f72da83207b77d2dcfb0b6fd063d4c"></a><a id="t.0"></a><table class="c52 c18"><tbody><tr class="c36"><td class="c67" colspan="1" rowspan="1"><p class="c5 c18"><span class="c16">tf.saved_model.loader.load(sess</span><span class="c11">, </span><span class="c16">[</span><span class="c17">&#39;vgg16&#39;</span><span class="c16">]</span><span class="c11">, </span><span class="c2">vgg_path)</span></p><p class="c5 c18"><span class="c2">graph = tf.get_default_graph()</span></p><p class="c0"><span class="c2"></span></p><p class="c5 c18"><span class="c16">image_input = graph.get_tensor_by_name(</span><span class="c17">&#39;image_input:0&#39;</span><span class="c2">)</span></p><p class="c5 c18"><span class="c16">keep_prob = graph.get_tensor_by_name(</span><span class="c17">&#39;keep_prob:0&#39;</span><span class="c2">)</span></p><p class="c5 c18"><span class="c16">layer3_out = graph.get_tensor_by_name(</span><span class="c17">&#39;layer3_out:0&#39;</span><span class="c2">)</span></p><p class="c5 c18"><span class="c16">layer4_out = graph.get_tensor_by_name(</span><span class="c17">&#39;layer4_out:0&#39;</span><span class="c2">)</span></p><p class="c5 c18"><span class="c16">layer7_out = graph.get_tensor_by_name(</span><span class="c17">&#39;layer7_out:0&#39;</span><span class="c16">)</span></p></td></tr></tbody></table><p class="c5 c40"><span class="c12"></span></p><h2 class="c24" id="h.ohi920t1fxhj"><span class="c19">2. The function layers is implementation </span></h2><p class="c5"><span>The authors of </span><span class="c23"><a class="c8" href="https://www.google.com/url?q=https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&amp;sa=D&amp;ust=1522472818111000">original paper</a></span><span class="c12">&nbsp;highly suggest to use these skip connections to improve segmentation accuracy.</span></p><p class="c5"><span class="c12">The original FCN-8s was trained in stages. The authors later uploaded a version that was trained all at once to their GitHub repo. The version in the GitHub repo has one important difference: The outputs of pooling layers 3 and 4 are scaled before they are fed into the 1x1 convolutions. As a result, some people have found that the model learns much better with the scaling layers included. The model may not converge substantially faster, but may reach a higher IoU and accuracy.</span></p><p class="c5 c40"><span class="c12"></span></p><p class="c5"><span>The function layers is implementation (see main.py ln78) as specified in the </span><span class="c23"><a class="c8" href="https://www.google.com/url?q=https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&amp;sa=D&amp;ust=1522472818112000">Fully Convolutional Networks for Semantic Segmentation</a></span><span class="c12">. In order for weights to be transposed convolution layers &nbsp;are implemented using tf.layers.conv2d. After a transpose layer I applied a skip technique by adding to the output of the upper layer. The purpose of the transpose layer to match upper layer shape so I can merge weights using tf.add. </span></p><a id="t.eed9f02870b85317a8244927041917418a942c06"></a><a id="t.1"></a><table class="c18 c52"><tbody><tr class="c36"><td class="c65" colspan="1" rowspan="1"><p class="c5 c18"><span class="c16">kernel_initializer = tf.truncated_normal_initializer(</span><span class="c13">stddev</span><span class="c16">=</span><span class="c35">self</span><span class="c16">.init_sd)</span></p><p class="c5 c18"><span class="c20"># 1x1 convolutions of the three layers</span></p><p class="c5 c18"><span class="c16">conv_7 = tf.layers.conv2d(vgg_layer7_out</span><span class="c11">, </span><span class="c16">num_classes</span><span class="c11">, </span><span class="c33">1</span><span class="c11">, </span><span class="c33">1</span><span class="c11">, &nbsp;</span><span class="c13">kernel_initializer</span><span class="c16">=kernel_initializer</span><span class="c11">, </span><span class="c13">kernel_regularizer</span><span class="c2">=kernel_regularizer)</span></p><p class="c5 c18"><span class="c16">conv_4 = tf.layers.conv2d(vgg_layer4_out</span><span class="c11">, </span><span class="c16">num_classes</span><span class="c11">, </span><span class="c33">1</span><span class="c11">, </span><span class="c33">1</span><span class="c30 c11">,</span></p><p class="c5 c18"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c13">kernel_initializer</span><span class="c16">=kernel_initializer</span><span class="c11">, </span><span class="c13">kernel_regularizer</span><span class="c2">=kernel_regularizer)</span></p><p class="c5 c18"><span class="c16">conv_3 = tf.layers.conv2d(vgg_layer3_out</span><span class="c11">, </span><span class="c16">num_classes</span><span class="c11">, </span><span class="c33">1</span><span class="c11">, </span><span class="c33">1</span><span class="c30 c11">,</span></p><p class="c5 c18"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c13">kernel_initializer</span><span class="c16">=kernel_initializer</span><span class="c11">, </span><span class="c13">kernel_regularizer</span><span class="c2">=kernel_regularizer)</span></p><p class="c0"><span class="c2"></span></p><p class="c5"><span class="c42">&nbsp;</span><span class="c30 c42"># Upsample layer 7 and add to layer 4</span></p><p class="c5 c18"><span class="c20"># tf.layers.conv2d_transpose(inputs,filters,kernel_size,strides=(1, 1), padding=&#39;valid&#39;...)</span></p><p class="c5 c18"><span class="c16">input = tf.layers.conv2d_transpose(conv_7</span><span class="c11">, </span><span class="c16">num_classes</span><span class="c11">, </span><span class="c33">4</span><span class="c11">, </span><span class="c33">2</span><span class="c11">, </span><span class="c17">&#39;SAME&#39;</span><span class="c30 c11">,</span></p><p class="c5 c18"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13">kernel_initializer</span><span class="c16">=kernel_initializer</span><span class="c11">, </span><span class="c13">kernel_regularizer</span><span class="c2">=kernel_regularizer)</span></p><p class="c5 c18"><span class="c16">input = tf.add(input</span><span class="c11">, </span><span class="c2">conv_4)</span></p><p class="c0"><span class="c2"></span></p><p class="c5 c18"><span class="c20"># add to layer 3</span></p><p class="c5 c18"><span class="c16">input = tf.layers.conv2d_transpose(input</span><span class="c11">, </span><span class="c16">num_classes</span><span class="c11">, </span><span class="c33">4</span><span class="c11">, </span><span class="c33">2</span><span class="c11">, </span><span class="c17">&#39;SAME&#39;</span><span class="c30 c11">,</span></p><p class="c5 c18"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13">kernel_initializer</span><span class="c16">=kernel_initializer</span><span class="c11">, </span><span class="c13">kernel_regularizer</span><span class="c2">=kernel_regularizer)</span></p><p class="c5 c18"><span class="c16">input = tf.add(input</span><span class="c11">, </span><span class="c2">conv_3)</span></p><p class="c0"><span class="c2"></span></p><p class="c5 c18"><span class="c20"># Upsample the input and return</span></p><p class="c5 c18"><span class="c16">input = tf.layers.conv2d_transpose(input</span><span class="c11">, </span><span class="c16">num_classes</span><span class="c11">, </span><span class="c33">16</span><span class="c11">, </span><span class="c33">8</span><span class="c11">, </span><span class="c17">&#39;SAME&#39;</span><span class="c30 c11">,</span></p><p class="c5 c18"><span class="c11">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c13">kernel_initializer</span><span class="c16">=kernel_initializer</span><span class="c11">, </span><span class="c13">kernel_regularizer</span><span class="c16">=kernel_regularizer)</span></p></td></tr><tr class="c36"><td class="c65" colspan="1" rowspan="1"><p class="c5 c40"><span class="c2"></span></p></td></tr></tbody></table><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c2"></span></p><p class="c5"><span class="c12">To smooth out on the edges pixels I ran a series of test with kernel_regularizer and &nbsp;kernel_initializer. And as you can see in the images bellow kernel_regularizer did slightly better. </span></p><p class="c5 c40"><span class="c2"></span></p><a id="t.e95b87504afaff215cb1e04a22cfcd5c6b8efa64"></a><a id="t.2"></a><table class="c52 c71"><tbody><tr class="c51"><td class="c59" colspan="2" rowspan="1"><p class="c15"><span class="c12">The following images were taking at 30 epoch just to identified a good way to smooth pixel on the edges. The real training model was trained with 100 epochs </span></p></td></tr><tr class="c36"><td class="c54" colspan="1" rowspan="1"><p class="c15"><span class="c12">With regularizer, no kernel_initialize</span></p></td><td class="c48" colspan="1" rowspan="1"><p class="c15"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 525.56px; height: 146.50px;"><img alt="" src="images/image8.png" style="width: 525.56px; height: 146.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td></tr><tr class="c6"><td class="c54" colspan="1" rowspan="1"><p class="c15"><span>With regularizer and kernel_initializer </span><span class="c70">vector norms</span><span class="c12">. </span></p></td><td class="c48" colspan="1" rowspan="1"><p class="c41 c18"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 513.00px; height: 142.67px;"><img alt="" src="images/image6.png" style="width: 513.00px; height: 142.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td></tr><tr class="c6"><td class="c54" colspan="1" rowspan="1"><p class="c15"><span>With kernel_initialize with </span><span class="c70">vector norms</span><span class="c12">, no regularizer</span></p></td><td class="c48" colspan="1" rowspan="1"><p class="c18 c41"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 513.00px; height: 142.67px;"><img alt="" src="images/image5.png" style="width: 513.00px; height: 142.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td></tr></tbody></table><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c12"></span></p><h3 class="c25 c18" id="h.vr2e46f1e67n"><span class="c3">3. Optimize the neural network</span></h3><h2 class="c24 c18" id="h.2zl938xw6jfi"><span>The function optimize is implemented correctly. </span><span class="c9 c44">We compute the softmax cross entropy between logits and labels and use an Adam algorithm optimizer to minimize the cross entropy loss.</span><span class="c30 c43">An important point to note is, batch size and learning rate are linked. If the batch size is too small then the gradients will become more unstable and would need to reduce the learning rate.</span></h2><p class="c0"><span class="c30 c9 c44"></span></p><p class="c0"><span class="c30 c9 c44"></span></p><a id="t.37483edd5c864963827a6c5c12b6813bdded772a"></a><a id="t.3"></a><table class="c52 c18"><tbody><tr class="c36"><td class="c68" colspan="1" rowspan="1"><p class="c5 c18"><span class="c20"># Reshape logits for computing cross entropy</span></p><p class="c5 c18"><span class="c16">logits = tf.reshape(nn_last_layer</span><span class="c11">, </span><span class="c16">(-</span><span class="c33">1</span><span class="c11">, </span><span class="c16">num_classes)</span><span class="c11">, </span><span class="c13">name</span><span class="c16">=</span><span class="c17">&#39;logits&#39;</span><span class="c2">)</span></p><p class="c0"><span class="c2"></span></p><p class="c5 c18"><span class="c20"># Compute cross entropy and loss</span></p><p class="c5 c18"><span class="c16">cross_entropy_logits = tf.nn.softmax_cross_entropy_with_logits(</span><span class="c13">logits</span><span class="c16">=logits</span><span class="c11">, </span><span class="c13">labels</span><span class="c2">=correct_label)</span></p><p class="c0"><span class="c2"></span></p><p class="c5 c18"><span class="c20"># All regularization terms are added to a collection called tf.GraphKeys.REGULARIZATION_LOSSES,</span></p><p class="c5 c18"><span class="c20"># add the sum of all regularization losses to the previously calculated cross-entropy</span></p><p class="c5 c18"><span class="c16">cross_entropy_loss = tf.reduce_mean(cross_entropy_logits) + &nbsp;</span><span class="c45">sum</span><span class="c2">(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))</span></p><p class="c0"><span class="c2"></span></p><p class="c5 c18"><span class="c20"># Training operation using the Adam optimizer</span></p><p class="c5 c18"><span class="c2">train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)</span></p><p class="c0"><span class="c20"></span></p></td></tr></tbody></table><p class="c0"><span class="c30 c9 c44"></span></p><p class="c5 c40"><span class="c12"></span></p><h3 class="c25 c18" id="h.lzvx64ibazim"><span class="c3">3. Train the neural network</span></h3><p class="c0"><span class="c12"></span></p><a id="t.a03e8e1ef417be7deaabefa86e9ff11945261503"></a><a id="t.4"></a><table class="c52 c69"><tbody><tr class="c51"><td class="c57" colspan="1" rowspan="1"><p class="c41 c18"><span class="c12">Below you can see the loss decreasing </span></p></td><td class="c57" colspan="1" rowspan="2"><p class="c18 c26"><span class="c12"></span></p><p class="c5 c18"><span>The function train_nn is implemented correctly. The loss of the network is as shown below is printed while the network is training. On average, the model decreases loss over time. </span><span class="c9">In my case the optimal epoch I found with my model perform better using a batch size of 10 and loose was did not reduced after 100 epochs. </span></p><p class="c0"><span class="c12"></span></p></td></tr><tr class="c51"><td class="c57" colspan="1" rowspan="1"><p class="c5 c18"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 205.03px; height: 305.50px;"><img alt="" src="images/image7.png" style="width: 205.03px; height: 305.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td></tr></tbody></table><p class="c31"><span class="c12"></span></p><h3 class="c18 c25" id="h.50zbpz6dluhj"><span class="c3">4. Verify results</span></h3><p class="c5 c18"><span class="c12">The project labels most pixels of roads close to the best solution. The model doesn&#39;t have to predict correctly all the images, just most of them. As you can see in the images bellow this model is labeling more than 80% of the road and label no more than 20% of non-road pixels as road. </span></p><p class="c0"><span class="c12"></span></p><a id="t.34c968869fbbdaac8fff0fec79919d98042a1192"></a><a id="t.5"></a><table class="c52 c18"><tbody><tr class="c36"><td class="c32" colspan="1" rowspan="1"><p class="c41"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 373.00px; height: 104.00px;"><img alt="" src="images/image2.png" style="width: 373.00px; height: 104.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c41"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 373.00px; height: 104.00px;"><img alt="" src="images/image10.png" style="width: 373.00px; height: 104.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td></tr><tr class="c36"><td class="c32" colspan="1" rowspan="1"><p class="c41"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 373.00px; height: 104.00px;"><img alt="" src="images/image3.png" style="width: 373.00px; height: 104.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c41"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 373.00px; height: 104.00px;"><img alt="" src="images/image1.png" style="width: 373.00px; height: 104.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td></tr></tbody></table><p class="c0"><span class="c12"></span></p><p class="c0"><span class="c12"></span></p><hr><p class="c46 c18 c40"><span class="c34"></span></p><h2 class="c24" id="h.ns5pwmt1kxgq"><span class="c19">Update:</span></h2><p class="c5"><span class="c43">Semantic Segmentation has evolved quite a lot, since FCNs came by. I would highly recommend reading, </span><span class="c21"><a class="c8" href="https://www.google.com/url?q=http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review&amp;sa=D&amp;ust=1522472818135000">Guide to Semantic Segmentation with Deep Learning</a></span><span class="c30 c43">, to explore how the solutions evolved since FCNs and the current state of the art used in real world problems.</span></p><p class="c1"><span class="c43">Do check out </span><span class="c39"><a class="c8" href="https://www.google.com/url?q=http://techtalks.tv/talks/fully-convolutional-networks-for-semantic-segmentation/61606/&amp;sa=D&amp;ust=1522472818136000">this talk</a></span><span class="c43">&nbsp;from authors of the original paper and also the </span><span class="c39"><a class="c8" href="https://www.google.com/url?q=https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&amp;sa=D&amp;ust=1522472818136000">corresponding paper</a></span><span class="c30 c43">&nbsp;for more in depth details.</span></p><p class="c1"><span class="c43 c62">Further Experimentation</span><span class="c30 c43">: If you wish to work on a challenging dataset, you&#39;ll enjoy Cityscapes dataset. It has fine image annotations for 29 classes of objects. The images are video frames taken in German cities and there is around 11GB of them. This sample comes from the City Scapes dataset.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 600.00px; height: 300.00px;"><img alt="" src="images/image4.png" style="width: 600.00px; height: 300.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c24 c18" id="h.kpumfxhu7sl8"><span class="c19">References:</span></h2><ul class="c50 lst-kix_a102oay53e4d-0 start"><li class="c5 c55"><span>Udacity Self-Driving Car <br>The link for the frozen VGG16 model is hardcoded into helper.py. The model can be found </span><span class="c23 c66"><a class="c8" href="https://www.google.com/url?q=https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip&amp;sa=D&amp;ust=1522472818137000">here</a></span></li><li class="c5 c55"><span>The model is not vanilla VGG16, but a fully convolutional version, which already contains the 1x1 convolutions to replace the fully connected layers. </span><span class="c60">The pretrained VGG-16 model is already fully convolutionalized, i.e. it already contains the 1x1 convolutions that replace the fully connected layers.</span></li><li class="c5 c55"><span class="c12">When adding l2-regularization, setting a regularizer in the arguments of the tf.layers is not enough. Regularization loss terms must be manually added to your loss function. otherwise regularization is not implemented.</span></li></ul></body></html>